# Data Foundry Pipeline

## Strategic Malayalam Dataset Integration for AI IVR

The **Data Foundry** is a comprehensive 4-phase pipeline designed to integrate 9 strategic Malayalam datasets from Hugging Face into your AI IVR's training infrastructure. This isn't a direct data feedâ€”it's an intelligent offline processing system that transforms raw datasets into production-ready AI model improvements.

### ğŸ¯ The Strategic Vision

Instead of overwhelming your live IVR with raw data, the Data Foundry creates a **controlled pipeline** that:
- âœ… **Securely ingests** Malayalam datasets from Hugging Face
- âœ… **Standardizes formats** for optimal AI training  
- âœ… **Routes data strategically** to specific AI engines
- âœ… **Fine-tunes models** using efficient LoRA adapters
- âœ… **Deploys safely** with shadow testing and monitoring

### ğŸ“Š Target Datasets Integration

| Dataset | Target Engine | Strategic Value | Priority |
|---------|---------------|-----------------|----------|
| `Be-win/IndicST-malayalam-only` | **STT Engine** | Core Malayalam speech recognition | ğŸ”´ High |
| `ayush-shunyalabs/malayalam-speech-dataset` | **STT Engine** | Enhanced vocabulary coverage | ğŸ”´ High |
| `CXDuncan/Malayalam-IndicVoices` | **STT Engine** | Voice diversity and natural speech | ğŸŸ¡ Medium |
| `Aby003/Malayalam_Dialects` | **Dialect Engine** | Travancore/Malabar/Cochin recognition | ğŸ”´ **Critical** |
| `Praha-Labs/rasa-malayalam-nano-codec` | **NLU Engine** | Intent classification | ğŸ”´ High |
| `Sakshamrzt/IndicNLP-Malayalam` | **NLU Engine** | Deep linguistic understanding | ğŸ”´ High |
| `wlkla/Malayalam_first_ready_for_sentiment` | **Sentiment Engine** | Emotion detection | ğŸ”´ High |
| `Be-win/malayalam-speech-with-english-translation` | **Translation Engine** | Malayalam-English parallel corpus | ğŸŸ¡ Medium |
| `Praha-Labs/imasc_slr_Malayalam-nano-codec` | **TTS Engine** | High-quality speech synthesis | ğŸŸ¡ Medium |

## ğŸš€ Quick Start

### Prerequisites

1. **Python 3.8+** with virtual environment
2. **Hugging Face Token** (optional, for private datasets)
3. **GPU recommended** (for Phase 4 training)

### Installation

```bash
# Clone or navigate to the data-foundry directory
cd data-foundry

# Install dependencies
pip install -r requirements.txt

# Set up Hugging Face token (optional)
export HUGGINGFACE_TOKEN="your_hf_token_here"
```

### Execute Complete Pipeline

```bash
# Run all 4 phases
python run_data_foundry.py

# Run specific phases
python run_data_foundry.py --phases phase1 phase2

# Dry run to preview execution
python run_data_foundry.py --dry-run
```

## ğŸ“‹ The 4-Phase Architecture

### ğŸ”§ Phase 1: Automated Ingestion
**Goal:** Pull all 9 datasets from Hugging Face into secure, private infrastructure.

```bash
python phase1-ingestion/ingest_hf_data.py
```

**What it does:**
- âœ… Downloads 9 target Malayalam datasets from Hugging Face
- âœ… Validates data quality and Malayalam content ratio
- âœ… Stores in organized Raw Data Lake structure
- âœ… Creates comprehensive metadata for each dataset
- âœ… Handles authentication and error recovery

**Output:** Raw datasets stored in `./storage/raw/` with quality metrics

---

### ğŸ›ï¸ Phase 2: Data Standardization
**Goal:** Convert disparate dataset formats into unified standards for AI model training.

```bash
python phase2-preprocessing/standardize_data.py
```

**What it does:**
- âœ… **Audio Standardization:** 16kHz mono WAV format for all speech data
- âœ… **Text Normalization:** UTF-8 Malayalam with proper Unicode handling
- âœ… **Quality Filtering:** Remove low-quality or inappropriate samples
- âœ… **Format Unification:** Consistent schema across all dataset types
- âœ… **Cultural Validation:** Preserve Malayalam cultural context

**Output:** Standardized datasets in `./storage/silver/` ready for allocation

---

### ğŸ¯ Phase 3: Strategic Allocation  
**Goal:** Route each dataset to its optimal AI engine target with intelligent mapping.

```bash
python phase3-allocation/allocate_datasets.py
```

**Strategic Routing:**
- ğŸ¤ **STT Engine:** `IndicST-malayalam-only` + `malayalam-speech-dataset` + `Malayalam-IndicVoices`
- ğŸ—ºï¸ **Dialect Engine:** `Malayalam_Dialects` â†’ Creates LoRA adapters for regional dialects
- ğŸ§  **NLU Engine:** `rasa-malayalam-nano-codec` + `IndicNLP-Malayalam`
- ğŸ˜Š **Sentiment Engine:** `Malayalam_first_ready_for_sentiment`
- ğŸŒ **Translation Engine:** `malayalam-speech-with-english-translation`
- ğŸ”Š **TTS Engine:** `imasc_slr_Malayalam-nano-codec`

**Output:** Engine-specific optimized datasets in `./storage/gold/`

---

### ğŸš€ Phase 4: Fine-Tuning & Deployment
**Goal:** Update live models with new intelligence using safe deployment practices.

```bash
python phase4-finetuning/train_deploy_models.py
```

**What it does:**
- âœ… **LoRA Fine-tuning:** Efficient adaptation without full retraining
- âœ… **Continuous Integration:** Automated training, validation, and deployment
- âœ… **Shadow Deployment:** Test new models on 10% of traffic silently
- âœ… **Performance Monitoring:** Real-time metrics and automatic rollback
- âœ… **Model Registry:** Version control and deployment tracking

**Output:** Production-ready models deployed to your AI IVR system

## ğŸ“ Directory Structure

```
data-foundry/
â”œâ”€â”€ phase1-ingestion/
â”‚   â””â”€â”€ ingest_hf_data.py           # HuggingFace dataset ingestion
â”œâ”€â”€ phase2-preprocessing/ 
â”‚   â””â”€â”€ standardize_data.py         # Audio/text standardization
â”œâ”€â”€ phase3-allocation/
â”‚   â””â”€â”€ allocate_datasets.py        # Engine-specific routing
â”œâ”€â”€ phase4-finetuning/
â”‚   â””â”€â”€ train_deploy_models.py      # LoRA training & deployment
â”œâ”€â”€ storage/
â”‚   â”œâ”€â”€ raw/                        # Raw ingested datasets
â”‚   â”œâ”€â”€ silver/                     # Standardized datasets  
â”‚   â””â”€â”€ gold/                       # Engine-allocated datasets
â”œâ”€â”€ models/                         # Trained model storage
â”œâ”€â”€ config/                         # Configuration files
â”œâ”€â”€ run_data_foundry.py            # Master orchestrator
â”œâ”€â”€ requirements.txt               # Dependencies
â””â”€â”€ README.md                      # This file
```

## ğŸ”§ Configuration

### Environment Variables
```bash
export HUGGINGFACE_TOKEN="your_hf_token"     # For private datasets
export WANDB_API_KEY="your_wandb_key"        # For training monitoring  
export DATA_FOUNDRY_BASE_PATH="./data-foundry"  # Custom base path
```

### Custom Configuration
Create `config/data_foundry_config.json`:
```json
{
  "base_path": "./data-foundry",
  "storage": {
    "type": "local",
    "backup_enabled": true
  },
  "training": {
    "use_wandb": true,
    "gpu_enabled": true,
    "mixed_precision": true
  },
  "deployment": {
    "shadow_testing": true,
    "rollback_enabled": true
  }
}
```

## ğŸ“Š Monitoring & Reports

### Pipeline Reports
Each execution generates comprehensive reports:
- `pipeline_report_YYYYMMDD_HHMMSS.json` - Detailed execution log
- `latest_pipeline_summary.json` - Quick status overview

### Key Metrics Tracked
- **Dataset Quality:** Malayalam content ratio, completeness scores
- **Training Performance:** Loss curves, validation metrics, convergence
- **Deployment Health:** Success rates, latency, error rates
- **Cultural Appropriateness:** Malayalam cultural context preservation

## ğŸ›¡ï¸ Safety & Quality Assurance

### Data Quality Gates
- âœ… Minimum 70% Malayalam content requirement
- âœ… Cultural appropriateness validation
- âœ… Audio quality filtering (16kHz, noise levels)
- âœ… Text normalization validation

### Model Safety
- âœ… Validation threshold requirements (85% minimum)
- âœ… Shadow deployment with 10% traffic
- âœ… Automatic rollback on performance degradation
- âœ… Cultural bias monitoring

### Deployment Safety
- âœ… Staged rollout (Shadow â†’ Staging â†’ Production)
- âœ… Real-time performance monitoring
- âœ… Automatic rollback triggers
- âœ… Manual approval gates for production

## ğŸ”„ Integration with Existing AI IVR

The Data Foundry integrates seamlessly with your existing AI IVR infrastructure:

### Current Services Enhanced
- **`MalayalamSpeechToTextService`** â† Enhanced by STT + Dialect datasets
- **`MalayalamNLPService`** â† Enhanced by NLU + Sentiment datasets  
- **`MalayalamTextToSpeechService`** â† Enhanced by TTS datasets

### New Capabilities Added
- **Regional Dialect Recognition** (Travancore/Malabar/Cochin)
- **Advanced Sentiment Detection** for user frustration/satisfaction
- **Real-time Malayalam-English Translation**
- **Improved Cultural Context Understanding**

## ğŸ“ˆ Expected Results

### Phase 1-2 (Data Pipeline)
- âœ… **9 datasets successfully ingested** with quality validation
- âœ… **Standardized format compliance** across all data types
- âœ… **Cultural content preservation** with 90%+ Malayalam fidelity

### Phase 3-4 (AI Enhancement)
- âœ… **25-40% improvement** in Malayalam speech recognition accuracy
- âœ… **Regional dialect support** for Travancore/Malabar/Cochin users
- âœ… **Enhanced sentiment detection** for better user experience
- âœ… **Zero-downtime deployment** with shadow testing validation

## ğŸš¨ Troubleshooting

### Common Issues

**Dataset Download Failures:**
```bash
# Check Hugging Face token
python -c "from huggingface_hub import login; login()"

# Test dataset access
python -c "from datasets import load_dataset; print(load_dataset('Be-win/IndicST-malayalam-only', split='train[:5]'))"
```

**GPU/Training Issues:**
```bash
# Check GPU availability
python -c "import torch; print(f'CUDA: {torch.cuda.is_available()}, Devices: {torch.cuda.device_count()}')"

# Monitor training
pip install wandb
wandb login your_wandb_key
```

**Storage Space:**
```bash
# Check storage requirements
du -h storage/  # ~5-15GB for raw datasets
df -h          # Ensure sufficient disk space
```

### Performance Optimization

**For Limited Resources:**
- Run phases sequentially rather than parallel
- Use smaller batch sizes in training configuration  
- Enable gradient checkpointing for memory efficiency

**For High Performance:**
- Use multiple GPUs with `accelerate` configuration
- Enable mixed precision training
- Optimize data loading with multi-processing

## ğŸ¤ Contributing

### Adding New Datasets
1. Add dataset configuration to `DATASET_REGISTRY` in Phase 1
2. Create corresponding allocation strategy in Phase 3
3. Update training configuration for target engine in Phase 4

### Custom AI Engines
1. Extend `AIEngine` enum in Phase 3
2. Implement engine-specific optimization in `_process_for_target_engine`
3. Add training configuration in Phase 4

## ğŸ“ Support

For technical support or questions about the Data Foundry pipeline:
- **Technical Issues:** Check logs in `data_foundry_*.log` files
- **Dataset Issues:** Verify Hugging Face access and token permissions
- **Training Issues:** Monitor training logs and GPU utilization
- **Deployment Issues:** Check shadow deployment metrics and rollback logs

---

**ğŸ¯ Ready to Transform Your AI IVR?**

Execute the complete pipeline:
```bash
python run_data_foundry.py
```

Watch as your AI IVR gains deep Malayalam understanding with regional dialect support, enhanced sentiment detection, and cultural appropriatenessâ€”all while maintaining production safety through intelligent shadow deployment! ğŸš€