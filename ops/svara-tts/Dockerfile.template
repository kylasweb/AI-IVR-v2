# Dockerfile.template for svara-tts inference (CPU)
# Copy into the model inference repo and adjust paths as needed.

FROM python:3.11-slim

WORKDIR /app

# Install system deps required by many TTS stacks
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential git wget curl ca-certificates libsndfile1 \
    && rm -rf /var/lib/apt/lists/*

# Copy repo requirements or add minimal ones
# COPY requirements.txt ./
# RUN pip install --no-cache-dir -r requirements.txt

# Example: install common ML deps (adjust as necessary)
RUN pip install --no-cache-dir "torch>=2.0.0" "transformers>=4.30.0" fastapi uvicorn[standard] soundfile

# Copy inference code into image (user should copy repo contents here)
COPY . /app

# Expose port used by FastAPI/uvicorn
EXPOSE 5000

# Entrypoint (user should replace with real server command)
CMD ["uvicorn", "server:app", "--host", "0.0.0.0", "--port", "5000"]
